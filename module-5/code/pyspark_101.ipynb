{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfad75e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from platform import python_version\n",
    "# print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb0a9ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !conda install --yes --prefix {sys.prefix} pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d3ee111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87a03625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/03 08:07:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41abfda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02682|2021-01-01 00:33:44|2021-01-01 00:49:07|         230|         166|   null|\n",
      "|           HV0003|              B02682|2021-01-01 00:55:19|2021-01-01 01:18:21|         152|         167|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:23:56|2021-01-01 00:38:05|         233|         142|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:42:51|2021-01-01 00:45:50|         142|         143|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:48:14|2021-01-01 01:08:42|         143|          78|   null|\n",
      "|           HV0005|              B02510|2021-01-01 00:06:59|2021-01-01 00:43:01|          88|          42|   null|\n",
      "|           HV0005|              B02510|2021-01-01 00:50:00|2021-01-01 01:04:57|          42|         151|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:14:30|2021-01-01 00:50:27|          71|         226|   null|\n",
      "|           HV0003|              B02875|2021-01-01 00:22:54|2021-01-01 00:30:20|         112|         255|   null|\n",
      "|           HV0003|              B02875|2021-01-01 00:40:12|2021-01-01 00:53:31|         255|         232|   null|\n",
      "|           HV0003|              B02875|2021-01-01 00:56:45|2021-01-01 01:17:42|         232|         198|   null|\n",
      "|           HV0003|              B02835|2021-01-01 00:29:04|2021-01-01 00:36:27|         113|          48|   null|\n",
      "|           HV0003|              B02835|2021-01-01 00:48:56|2021-01-01 00:59:12|         239|          75|   null|\n",
      "|           HV0004|              B02800|2021-01-01 00:15:24|2021-01-01 00:38:31|         181|         237|   null|\n",
      "|           HV0004|              B02800|2021-01-01 00:45:00|2021-01-01 01:06:45|         236|          68|   null|\n",
      "|           HV0003|              B02682|2021-01-01 00:11:53|2021-01-01 00:18:06|         256|         148|   null|\n",
      "|           HV0003|              B02682|2021-01-01 00:28:31|2021-01-01 00:41:40|          79|          80|   null|\n",
      "|           HV0003|              B02682|2021-01-01 00:50:49|2021-01-01 00:55:59|          17|         217|   null|\n",
      "|           HV0005|              B02510|2021-01-01 00:08:40|2021-01-01 00:39:39|          62|          29|   null|\n",
      "|           HV0003|              B02836|2021-01-01 00:53:48|2021-01-01 01:11:40|          22|          22|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('fhvhv_tripdata_2021-01.csv')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a79fce14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "# We see that the columns are all in string. By default, Spark does not try to infer column types.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62df5e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime='2021-01-01 00:33:44', dropoff_datetime='2021-01-01 00:49:07', PULocationID='230', DOLocationID='166', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime='2021-01-01 00:55:19', dropoff_datetime='2021-01-01 01:18:21', PULocationID='152', DOLocationID='167', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime='2021-01-01 00:23:56', dropoff_datetime='2021-01-01 00:38:05', PULocationID='233', DOLocationID='142', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime='2021-01-01 00:42:51', dropoff_datetime='2021-01-01 00:45:50', PULocationID='142', DOLocationID='143', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime='2021-01-01 00:48:14', dropoff_datetime='2021-01-01 01:08:42', PULocationID='143', DOLocationID='78', SR_Flag=None)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2466d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create a file with only the first 1001 lines.\n",
    "!head -n 1001 fhvhv_tripdata_2021-01.csv > head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "064140f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 7 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   hvfhs_license_num     1000 non-null   object \n",
      " 1   dispatching_base_num  1000 non-null   object \n",
      " 2   pickup_datetime       1000 non-null   object \n",
      " 3   dropoff_datetime      1000 non-null   object \n",
      " 4   PULocationID          1000 non-null   int64  \n",
      " 5   DOLocationID          1000 non-null   int64  \n",
      " 6   SR_Flag               0 non-null      float64\n",
      "dtypes: float64(1), int64(2), object(4)\n",
      "memory usage: 54.8+ KB\n"
     ]
    }
   ],
   "source": [
    "#  Read this small file in pandas.\n",
    "df_pandas = pd.read_csv('head.csv')\n",
    "df_pandas.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27ec252b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afrogcp/tooling/spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/afrogcp/tooling/spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropoff_datetime', StringType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('SR_Flag', DoubleType(), True)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Spark DataFrame from a pandas.DataFrame.\n",
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c5b5a00",
   "metadata": {},
   "source": [
    "Spark provides `spark.sql.types.StructType` class to define the structure of the DataFrame and It is a collection or list on StructField objects.\n",
    "Spark provides `spark.sql.types.StructField` class to define the column name(String), column type (DataType), nullable column (Boolean) and metadata (MetaData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c62c00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-01.csv')\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52342e00",
   "metadata": {},
   "source": [
    "Save as partitioned parquet files\n",
    "\n",
    "Spark/PySpark partitioning is a way to split the data into multiple partitions so that you can execute transformations on multiple partitions in parallel which allows completing the job faster. See Spark Partitioning & Partition Understanding for more.\n",
    "\n",
    "Spark DataFrame repartition() method is used to increase or decrease the partitions. This is very expensive operation as it shuffle the data across many partitions hence try to minimize repartition as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "358643d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df.repartition(24)\n",
    "df.write.parquet('fhvhv/2021/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f515dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read the parquet files\n",
    "\n",
    "df = spark.read.parquet('fhvhv/2021/01/')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d647af25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+\n",
      "|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "|2021-01-05 22:14:07|2021-01-05 22:32:28|         189|         107|\n",
      "|2021-01-02 17:59:55|2021-01-02 18:10:39|          88|         137|\n",
      "|2021-01-02 23:57:54|2021-01-03 00:15:48|         238|         224|\n",
      "|2021-01-06 15:53:13|2021-01-06 16:07:07|         169|         208|\n",
      "|2021-01-07 07:35:24|2021-01-07 07:55:49|          75|          88|\n",
      "|2021-01-07 08:45:12|2021-01-07 08:51:17|         210|         210|\n",
      "|2021-01-02 15:44:26|2021-01-02 16:10:50|         243|          69|\n",
      "|2021-01-04 16:50:28|2021-01-04 16:57:43|         250|         213|\n",
      "|2021-01-03 10:30:34|2021-01-03 10:44:53|          87|          79|\n",
      "|2021-01-03 22:05:20|2021-01-03 22:27:55|          68|         181|\n",
      "|2021-01-04 08:01:02|2021-01-04 08:33:27|          95|         236|\n",
      "|2021-01-02 13:01:10|2021-01-02 13:08:11|         262|         236|\n",
      "|2021-01-06 17:12:27|2021-01-06 17:46:56|         237|          83|\n",
      "|2021-01-04 09:05:18|2021-01-04 09:27:50|         159|          75|\n",
      "|2021-01-06 16:46:47|2021-01-06 17:50:24|         109|         119|\n",
      "|2021-01-06 08:03:47|2021-01-06 08:17:43|         145|         229|\n",
      "|2021-01-04 06:45:42|2021-01-04 06:55:01|         250|         212|\n",
      "|2021-01-03 13:20:41|2021-01-03 13:31:11|         130|          28|\n",
      "|2021-01-03 17:30:33|2021-01-03 17:45:19|          81|          46|\n",
      "|2021-01-06 20:55:57|2021-01-06 21:02:01|         113|          79|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID') \\\n",
    "  .filter(df.hvfhs_license_num == 'HV0003').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fee932d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+\n",
      "|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-----------+------------+------------+------------+\n",
      "| 2021-01-03|  2021-01-03|         255|          34|\n",
      "| 2021-01-05|  2021-01-05|         189|         107|\n",
      "| 2021-01-02|  2021-01-02|          88|         137|\n",
      "| 2021-01-02|  2021-01-03|         238|         224|\n",
      "| 2021-01-06|  2021-01-06|         169|         208|\n",
      "| 2021-01-07|  2021-01-07|          75|          88|\n",
      "| 2021-01-07|  2021-01-07|         210|         210|\n",
      "| 2021-01-02|  2021-01-02|         243|          69|\n",
      "| 2021-01-04|  2021-01-04|         250|         213|\n",
      "| 2021-01-03|  2021-01-03|          87|          79|\n",
      "| 2021-01-03|  2021-01-03|          68|         181|\n",
      "| 2021-01-04|  2021-01-04|          95|         236|\n",
      "| 2021-01-02|  2021-01-02|         262|         236|\n",
      "| 2021-01-04|  2021-01-04|         225|         233|\n",
      "| 2021-01-06|  2021-01-06|         237|          83|\n",
      "| 2021-01-05|  2021-01-05|         231|          87|\n",
      "| 2021-01-06|  2021-01-06|          22|          26|\n",
      "| 2021-01-04|  2021-01-04|         159|          75|\n",
      "| 2021-01-06|  2021-01-06|         109|         119|\n",
      "| 2021-01-06|  2021-01-06|         145|         229|\n",
      "+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Functions avalaible in Spark\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    "    .select('pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
    "    .show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70d6d78e",
   "metadata": {},
   "source": [
    "#### User defined functions (UDF)\n",
    "User-Defined Functions (UDFs) are user-programmable routines that act on one row. See Scalar User Defined Functions (UDFs) and functions.udf for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18610316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+------------+------------+\n",
      "|base_id|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "|  e/9ce| 2021-01-03|  2021-01-03|         255|          34|\n",
      "|  e/b42| 2021-01-05|  2021-01-05|         189|         107|\n",
      "|  e/b33| 2021-01-02|  2021-01-02|          88|         137|\n",
      "|  e/b38| 2021-01-02|  2021-01-03|         238|         224|\n",
      "|  e/b3b| 2021-01-06|  2021-01-06|         169|         208|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def crazy_stuff(base_num):\n",
    "    num = int(base_num[1:])\n",
    "    if num % 7 == 0:\n",
    "        return f's/{num:03x}'\n",
    "    elif num % 3 == 0:\n",
    "        return f'a/{num:03x}'\n",
    "    else:\n",
    "        return f'e/{num:03x}'\n",
    "\n",
    "crazy_stuff('B02884')  # Return 's/b44'\n",
    "\n",
    "crazy_stuff_udf = F.udf(crazy_stuff, returnType=types.StringType())\n",
    "\n",
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    "    .withColumn('base_id', crazy_stuff_udf(df.dispatching_base_num)) \\\n",
    "    .select('base_id', 'pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
    "    .show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96376f8c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341c69ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "batch",
   "language": "python",
   "name": "batch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
